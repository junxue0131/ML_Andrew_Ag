# ex5练习总结

#### 关于正则化线性回归的梯度问题
* 关于正则化项数，对不同的θ求梯度，实际上是求不同θ的偏导，那么对于每一个偏导式，原来的式子就都是来源于J代价函数的。
* 因此在进行梯度计算的时候，均是对整个J代价函数进行求导的，不同的地方在于，每个偏导式的x乘数项对应着原θ对应着的那一列x值（x一般来说是一个矩阵），后面的正则化修正项也是一样，也进行对应的求导，只对所求的那一个θ求导，其他的都当常数项处理
* 另外，在进行梯度计算的时候，利用矩阵直接计算会让整个程序和算法显得更简洁，例如：
```
temp = theta;

temp(1, 1) = 0;

grad = (1 / m) * sum((h - y) .* X) + lambda / m * temp';
```
直接计算出grad的矩阵形式，不用分情况讨论，也不用作合并计算，只需要注意在计算前将第一个theta置为0，保证第一个theta不参与正则化的计算

#### 关于学习曲线
* 学习曲线就是绘出，在递增样本数下，训练集误差和验证集误差的变化情况，并根据这种变化来推测是高偏差还是高方差，因此在计算中需要每增加一些样本数，就重新梯度计算θ一次，计算误差一次，直到绘出相应的函数图象
